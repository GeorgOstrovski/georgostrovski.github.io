<head>
<title>Georg Ostrovski</title>
<link href="stylesheet.css" rel="stylesheet" type="text/css" />
</head>

<body>
<center><h1>Georg Ostrovski</h1>
<hr WIDTH="30%" SIZE="6" NOSHADE="NOSHADE">
</center>

<h2>About me</h2>
<img src="data/pic.jpg" alt="Me" width="300" height="413" align="right">
<p>I am a Research Engineering Team Lead at <a href="http://deepmind.com/">DeepMind</a> in London, working on problems related to Deep Reinforcement Learning.</p>

<p>Before that, I completed a PhD in mathematics under the supervision of <a href="http://www2.imperial.ac.uk/~svanstri/">Prof. Sebastian van Strien</a> at the University of Warwick. My PhD research revolved around Dynamical Systems in Game Theory, in particular
<ul>
    <li>Fictitious Play Dynamics and related learning processes in games (their asymptotic properties, convergence rates, combinatorial description, emergence of chaos)
    <li>the dynamics of piecewise affine maps and flows (combinatorial description, ergodic properties)
</ul>
</p>

<h2>Publications</h2>

<h3> 2021 </h3>

<ul>

<li>
<b><a href="https://arxiv.org/pdf/2110.14020.pdf">
The Difficulty of Passive Learning in Deep Reinforcement Learning
</a></b>
<br>G. Ostrovski, P. S. Castro, W. Dabney
<i>arXiv preprint, 2021</i> 
(<a href=https://arxiv.org/abs/2110.14020>arXiv</a>; 
<a href="https://psc-g.github.io/posts/research/rl/tandem/">P.S. Castro's blog post</a>)
</li>
<br>

<li>
<b><a href="https://arxiv.org/pdf/2108.11811.pdf">
When should agents explore?
</a></b>
<br>M. Pislar, D. Szepesvari, G. Ostrovski, D. Borsa, T. Schaul.
<i>arXiv preprint, 2021</i> 
(<a href=https://arxiv.org/abs/2108.11811>arXiv</a>)
</li>
<br>

<li>
<b><a href="https://arxiv.org/pdf/2105.05347.pdf">
Return-based Scaling: Yet Another Normalisation Trick for Deep RL
</a></b>
<br>T. Schaul, G. Ostrovski, Iurii Kemaev, D. Borsa.
<i>arXiv preprint, 2021</i> 
(<a href=https://arxiv.org/abs/2105.05347>arXiv</a>)
</li>
<br>

<li>
<b><a href="https://arxiv.org/pdf/2102.13089.pdf">
On The Effect of Auxiliary Tasks on Representation Dynamics
</a></b>
<br>C. Lyle, M. Rowland, G. Ostrovski, W. Dabney.
<i>Proceedings of the 24<sup>th</sup> International Conference on Artificial Intelligence and Statistics (AISTATS) 2021, San Diego, California, USA, PMLR 130, 2021</i> 
(<a href="http://proceedings.mlr.press/v130/lyle21a/lyle21a.pdf">conference paper</a>; <a href=https://arxiv.org/abs/2102.13089>arXiv</a>)
</li>
<br>

<li>
<b><a href="https://openreview.net/pdf?id=ONBPHFZ7zG4">
Temporally-Extended ε-Greedy Exploration
</a></b>
<br>W. Dabney, G. Ostrovski, A. Barreto.
<i>9<sup>th</sup> International Conference on Learning Representations (ICLR'21), 2021</i> 
(<a href=https://openreview.net/forum?id=ONBPHFZ7zG4>conference paper</a>, 
<a href=https://arxiv.org/abs/2006.01782>arXiv</a>)
</li>

</ul>


<h3> 2019 </h3>
<ul>

<li>
<b><a href="https://arxiv.org/abs/1912.06910">Adapting Behaviour for Learning Progress</a></b>
<br>T. Schaul, D. Borsa, D. Ding, D. Szepesvari, G. Ostrovski, W. Dabney, S. Osindero.
<i>arXiv preprint, 2019</i> (<a href="https://arxiv.org/abs/1912.06910">arXiv</a>)
</li>
<br>

<li>
<b><a href="https://openreview.net/pdf?id=r1lyTjAqYX">
Recurrent Experience Replay in Distributed Reinforcement Learning
</a></b>
<br>S. Kapturowski, G. Ostrovski, J. Quan, R. Munos, W. Dabney.
<i>7<sup>th</sup> International Conference on Learning Representations (ICLR'19), 2019</i>
(<a href="https://openreview.net/forum?id=r1lyTjAqYX">conference paper</a>)
</li>

</ul>


<h3> 2018 </h3>
<ul>

<li>
<b><a href="https://arxiv.org/pdf/1806.05575.pdf">
Autoregressive Quantile Networks for Generative Modeling
</a></b>
<br>G. Ostrovski, W. Dabney, R. Munos.
<i>Proceedings of the 35<sup>th</sup> International Conference on Machine Learning (ICML'18), Stockholm, Sweden, PMLR 80, 2018</i>
(<a href="http://proceedings.mlr.press/v80/ostrovski18a/ostrovski18a.pdf">conference paper</a>, 
<a href="http://proceedings.mlr.press/v80/ostrovski18a/ostrovski18a-supp.pdf">supplementary</a>,
<a href="https://arxiv.org/abs/1806.05575">arXiv</a>)
</li>
<br>

<li>
<b><a href="https://arxiv.org/pdf/1806.06923.pdf">
Implicit Quantile Networks for Distributional Reinforcement Learning
</a></b>
<br>W. Dabney, G. Ostrovski, D. Silver, R. Munos.
<i>Proceedings of the 35<sup>th</sup> International Conference on Machine Learning (ICML'18), Stockholm, Sweden, PMLR 80, 2018</i>
(<a href="http://proceedings.mlr.press/v80/dabney18a/dabney18a.pdf">conference paper</a>, 
<a href="http://proceedings.mlr.press/v80/dabney18a/dabney18a-supp.pdf">supplementary</a>,
<a href="https://arxiv.org/abs/1806.06923">arXiv</a>)
</li>
<br>

<li>
<b><a href="https://www.nature.com/articles/s41598-018-19194-4.pdf">
Symmetric Decomposition of Asymmetric Games
</a></b>
<br>K. Tuyls, J. Pérolat, M. Lanctot, G. Ostrovski, R. Savani, J. Z. Leibo, T. Ord, T. Graepel, S. Legg.
<i>Scientific Reports 8, 2018</i>
(<a href="https://www.nature.com/articles/s41598-018-19194-4">journal paper</a>, <a href="https://arxiv.org/abs/1711.05074">arXiv</a>)
</li>
<br>

<li>
<b><a href="https://arxiv.org/pdf/1710.02298.pdf">
Rainbow: Combining Improvements in Deep Reinforcement Learning
</a></b>
<br>M. Hessel, J. Modayil, H. van Hasselt, T. Schaul, G. Ostrovski, W. Dabney,
D. Horgan, B. Piot, M. Azar, D. Silver.
<i>Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence
(AAAI), 2018</i>
(<a href="http://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17204/16680">conference paper</a>, 
<a href="https://arxiv.org/abs/1710.02298">arXiv</a>)
</li>

</ul>


<h3> 2017 </h3>
<ul>

<li>
<b><a href="https://arxiv.org/pdf/1703.01310.pdf">
Count-Based Exploration with Neural Density Models
</a></b>
<br>G. Ostrovski, M. G. Bellemare, A. van den Oord, R. Munos.
<i>Proceedings of the 34<sup>th</sup> International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017</i>
(<a href="http://proceedings.mlr.press/v70/ostrovski17a/ostrovski17a.pdf">conference paper</a>, <a href="http://proceedings.mlr.press/v70/ostrovski17a/ostrovski17a-supp.pdf">supplementary</a>, <a href="https://arxiv.org/abs/1703.01310">arXiv</a>)
</li>

</ul>

<h3> 2016 </h3>
<ul>

<li>
<b><a href="https://www.nature.com/articles/nature20101.pdf">
Hybrid computing using a neural network with dynamic external memory</a></b>
<br>
A. Graves, G. Wayne, M. Reynolds, T. Harley, I. Danihelka, A. Grabska-Barwińska,
S. Gómez Colmenarejo, E. Grefenstette, T. Ramalho, J. Agapiou,
A. Puigdomènech Badia, K. M. Hermann, Y. Zwols, G. Ostrovski, A. Cain, H. King,
C. Summerfield, P. Blunsom, K. Kavukcuoglu, D. Hassabis.
<i>Nature, 2016, Volume 538, pp.471-476</i>
(<a href="http://dx.doi.org/10.1038/nature20101">journal paper</a>)
</li>
<br>

<li>
<b><a href="https://arxiv.org/pdf/1606.01868.pdf">
Unifying Count-Based Exploration and Intrinsic Motivation
</a></b>
<br>M. G. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, R. Munos.
<i>Advances in Neural Information Processing Systems 29 (NIPS 2016)</i>
(<a href="http://papers.nips.cc/paper/6383-unifying-count-based-exploration-and-intrinsic-motivation.pdf">conference paper</a>, <a href="https://arxiv.org/abs/1606.01868">arXiv</a>)
</li>
<br>

<li>
<b><a href="https://arxiv.org/pdf/1512.04860.pdf">
Increasing the Action Gap: New Operators for Reinforcement Learning</a></b>
<br>M. G. Bellemare, G. Ostrovski, A. Guez, P. S. Thomas, R. Munos.
<i>Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence
(AAAI 2016)</i>
(<a href="http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12428/11761">conference paper</a>,
<a href="https://arxiv.org/abs/1512.04860">arXiv</a>)
</li>

</ul>

<h3> 2015 </h3>
<ul>

<li>
<b><a href="https://www.nature.com/articles/nature14236.pdf">
Human-level control through deep reinforcement learning</a></b>
<br>V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen,
C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra,
S. Legg, D. Hassabis.
<i>Nature, 2015, Volume 518, pp.529-533</i>
(<a href="http://dx.doi.org/10.1038/nature14236">journal paper</a>)
</li>

</ul>

<h3> 2014 </h3>
<ul>

<li>
<b><a href="https://arxiv.org/pdf/1308.4049.pdf">
Payoff Performance of Fictitious Play</a></b>
<br>G. Ostrovski, S. van Strien.
<i>Journal of Dynamics and Games, 2014, Volume 1, Number 4, pp.621-638</i>
(<a href="http://dx.doi.org/10.3934/jdg.2014.1.621">journal paper</a>, 
<a href="https://arxiv.org/abs/1308.4049">arXiv</a>)
</li>
<br>

<li>
<b><a href="https://arxiv.org/pdf/1305.4282.pdf">
Dynamics of a Continuous Piecewise Affine Map of the Square</a></b>
<br>G. Ostrovski.
<i>Physica D: Nonlinear Phenomena, 2014, Volume 271, pp.1-9</i>
(<a href="http://www.sciencedirect.com/science/article/pii/S0167278913003436">journal paper</a>, 
<a href="https://arxiv.org/abs/1305.4282">arXiv</a>)
</li>

</ul>

<h3> 2013 </h3>
<ul>

<li>
<b><a href="/data/thesis.pdf">Topics arising from Fictitious Play Dynamics</a></b>
<br>G. Ostrovski. <i>Ph.D. Thesis, 2013, University of Warwick</i>
</li>
<br>

<li><b><a href="https://arxiv.org/pdf/1112.3587.pdf">
Fixed Point Theorem for Non-Self Maps of Regions in the Plane</a></b>
<br>G. Ostrovski
<i>Topology and its Applications, 2013, Vol 160, Issue 7, pp.915-923</i>
(<a href="http://dx.doi.org/10.1016/j.topol.2013.03.004">journal paper</a>, 
<a href="https://arxiv.org/abs/1112.3587">arXiv</a>)</li>
<br>

</ul>

<h3> 2011 </h3>
<ul>

<li><b><a href="https://arxiv.org/pdf/1011.2018.pdf">
Piecewise Linear Hamiltonian Flows Associated to Zero-Sum Games:
Transition Combinatorics and Questions on Ergodicity</a></b>
<br>G. Ostrovski, S. van Strien.
<i>Regular and Chaotic Dynamics, 2011, Volume 17, pp.129-154</i>
(<a href="http://dx.doi.org/10.1134/S1560354711010059">journal paper</a>, 
<a href="https://arxiv.org/abs/1011.2018">arXiv</a>)</li>
</ul>


<h2>Presentations and Organized Meetings</h2>

I have given several talks on topics related to my work
in Reinforcement Learning:
<ul>
<li>11 June 2018: "Curiosity-Based Exploration in Deep Reinforcement Learning",
<a href="http://imperialsiam.com/2018/05/19/siam-annual-conference-2018/">SIAM Annual Conference of the SIAM student chapter</a>,
Imperial College London</li>
<li>22 March 2018: "Exploration in Deep Reinforcement Learning",
Dynamical Systems Seminar, Imperial College London</li>
</ul>

In the years of my PhD I have given a number of talks on my research in
Dynamical Systems and Game Theory:
<ul>
<li>12 March 2013: "A Dynamical System Motivated by
Games: Fictitious Play and Piecewise Affine Dynamics", Dynamical Systems and
Statistical Physics Seminar, Queen Mary University of London</li>
<li>9 May 2012: "Arnold Diffusion in Fictitious Play", Workshop <a href="http://www2.warwick.ac.uk/fac/sci/maths/research/events/2011-2012/wkd/">"From mean-field control to weak KAM dynamics"</a>, University of Warwick</li>
<li>15 June 2011: "Learning Dynamics in Games", Maths Postgraduate Seminar, Queen Mary University of London</li>
<li>18 May 2011: "Learning Dynamics in Games: Fictitious Play", Maths Postgraduate Seminar, University of Warwick</li>
<li>15 March 2011: "Fictitious Play Dynamics", Ergodic Theory and Dynamical Systems Seminar, University of Warwick</li>
</ul>

I have co-organized (with Julia Slipantschuk) the <a href="http://www.maths.qmul.ac.uk/~juliasl/onedaydynamics.html">"One-Day Meeting for PhD Students in Dynamical Systems and Ergodic Theory"</a> (YRM Satellite Meeting), on 26 March 2012, at the University of Warwick.
<br><br><br>
<br style="clear:both">


<centre>
<a href="http://www.freedomain.co.nr/" target="_blank" title="Free Domain Name"><img src="http://umsaoaa.imdrv.net/but1.gif" width="88" height="31" border="0" alt="Free Domain Name" /></a>
</centre>

